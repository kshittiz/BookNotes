# Chapter 12: The Future of Data Systems

## Core Concept

> "The future is about combining specialized tools into cohesive data systems."

**What it is:** No single database does everything well. Modern systems combine multiple specialized tools (OLTP DB, search index, cache, analytics warehouse) connected by data pipelines.

**Why it matters:** You'll design systems that integrate 5+ data stores. Understanding how to keep them consistent and when to use each is critical.

---

## Data Integration: The Real Challenge

### The Reality of Production Systems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App writes â†’ PostgreSQL (source of truth)                â”‚
â”‚                    â”‚                                       â”‚
â”‚               CDC/Kafka                                    â”‚
â”‚                    â”‚                                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â–¼         â–¼          â–¼                            â”‚
â”‚     Redis     Elasticsearch  Snowflake                    â”‚
â”‚    (cache)     (search)     (analytics)                   â”‚
â”‚                                                            â”‚
â”‚  Challenge: How do you keep all these in sync?            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this happens:** Different access patterns need different data stores:
- Transactions â†’ PostgreSQL (ACID)
- Full-text search â†’ Elasticsearch
- Fast reads â†’ Redis
- Analytics â†’ Snowflake/BigQuery

> **ğŸ’¡ Interview Tip:** "Never dual-write (writing to two systems in application code). Use CDC or transactional outbox pattern to propagate changes reliably."

---

## Source of Truth vs Derived Data

**What it is:**
- **Source of truth:** The authoritative, primary copy (usually where writes happen)
- **Derived data:** Computed from source of truth, can be rebuilt

**Why this distinction matters:**
- Derived data can be wrong (rebuild from source to fix)
- Source of truth must never be corrupted
- Know which is which in your system!

```
Source of Truth          Derived Data
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PostgreSQL (writes)  â†’   Elasticsearch (search index)
Event log (Kafka)    â†’   Materialized views
                     â†’   Redis cache
                     â†’   Analytics aggregates

If search index is wrong: rebuild from source
If source is wrong: you have a real problem
```

> **ğŸ’¡ Interview Tip:** "I always identify the source of truth first. Everything else is derived and should be rebuildable. This makes recovery from bugs much easier."

---

## End-to-End Correctness

### The Problem: Guarantees Don't Compose

**What it is:** Each component might be "reliable," but the end-to-end system isn't automatically reliable.

```
Web â†’ API â†’ Queue â†’ Worker â†’ DB â†’ Search

Each hop says "I'm reliable!" but:
- API crashes after ACKing queue but before DB write
- DB writes but search update fails  
- Network timeout: did it succeed or not?
```

**Why it's hard:** "Reliable" components can fail between steps. Partial failures leave the system inconsistent.

### Solutions for End-to-End Exactly-Once

**1. Idempotency Keys (most common in practice)**

```python
# Client generates unique ID for each request
POST /orders
{
    "idempotency_key": "abc-123-xyz",  # Client generates
    "user_id": 42,
    "items": [...]
}

# Server checks before processing
if db.exists(idempotency_key="abc-123-xyz"):
    return previous_result  # Already processed, return same response
else:
    result = process_order()
    db.save(idempotency_key="abc-123-xyz", result)
    return result
```

**Why this works:** Retries with same key return same result. No duplicates.

**2. Transactional Outbox Pattern**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SINGLE TRANSACTION:                                        â”‚
â”‚  1. Write business data to orders table                    â”‚
â”‚  2. Write event to outbox table                            â”‚
â”‚  COMMIT (atomic - both or neither)                         â”‚
â”‚                                                             â”‚
â”‚  SEPARATE PROCESS:                                         â”‚
â”‚  Poll outbox â†’ Publish to Kafka â†’ Mark as sent            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this works:** No dual-write. Event publishing is eventually consistent but never loses events.

**3. Deterministic Processing**

Same input always produces same output. Reprocessing is safe because results are identical.

> **ğŸ’¡ Interview Tip:** "For payment systems, I'd use idempotency keys. Client generates a unique ID, server checks before processing. Retries are safe, and I can show customers 'your request was already processed.'"

---

## Timeliness vs Integrity

**What it is:**
- **Timeliness:** Users see up-to-date data (low latency)
- **Integrity:** Data is correct (no corruption, no constraint violations)

**The key insight:** You can recover from timeliness problems (eventual consistency catches up). You often can't recover from integrity problems (corrupted data).

```
Timeliness violation: "User sees stale data for 5 seconds"
â†’ Annoying but self-healing

Integrity violation: "Two users both got the same username"  
â†’ Manual intervention required, possibly forever broken
```

**Principle: Prioritize integrity over timeliness.**

Show stale data rather than corrupt data. A "slightly behind" cache is fine; a cache with wrong data is a bug.

> **ğŸ’¡ Interview Tip:** "I'd rather show 'data loading...' for 2 seconds than show wrong data instantly. Integrity bugs are much harder to fix than latency issues."

---

## ACID vs BASE: A Practical View

**What it is:** Two approaches to consistency in distributed systems.

**The reality:** It's not either/or. Use both where appropriate:

```
Writes (source of truth):
â”œâ”€â”€ Strong consistency for critical paths
â”œâ”€â”€ Idempotent operations
â””â”€â”€ Append-only log (immutable)

Reads (derived views):  
â”œâ”€â”€ Eventual consistency is fine
â”œâ”€â”€ Optimized for query patterns
â””â”€â”€ Rebuildable from source
```

**Example:** Payment processing is ACID (must be correct). Product recommendations can be BASE (slightly stale is fine).

---

## Constraints in Async Systems

**The challenge:** Traditional databases enforce constraints synchronously (check then insert, all in one transaction). In async systems, you can't do that.

**Three strategies:**

**1. Partition by constraint key**
```
Unique username constraint:
â†’ Partition registration requests by username
â†’ All requests for "alice" go to same partition
â†’ Single processor detects conflicts
```

**2. Compensating transactions**
```
Accept order optimistically
â†’ Later: check if inventory available
â†’ If not: send apology email, refund
(This is how airlines do overbooking!)
```

**3. Synchronous fallback for critical constraints**
```
Most operations: async, eventual consistency
Money transfer: sync, strong consistency
```

> **ğŸ’¡ Interview Tip:** "Not everything needs strong consistency. For usernames, I might check synchronously. For view counts, eventual consistency is fine. Match the consistency level to the business requirement."

---

## Lambda vs Kappa Architecture

### Lambda Architecture

**What it is:** Run both batch and stream processing, merge results.

```
        Raw Data
       /        \
  Batch Layer   Speed Layer
  (accurate)    (fast)
       \        /
     Serving Layer (merge)
```

**Problem:** Maintaining two codebases with same logic. Tests, bugs, deployments all doubled.

### Kappa Architecture

**What it is:** Stream processing only. For reprocessing, replay from the log.

```
Raw Events â†’ Stream Processor â†’ Views
                  â†‘
           Replay to reprocess
```

**Why Kappa often wins:** 
- Single codebase
- Kafka retains events for replay
- Flink/Spark Streaming handle both real-time and batch

> **ğŸ’¡ Interview Tip:** "I'd start with Kappa architecture. Keep a long retention in Kafka, and reprocess by replaying. Lambda's dual codebases cause more problems than they solve."

---

## Data Mesh

**What it is:** Decentralized data ownership. Domain teams own their data as a "product," not a central data team.

```
Traditional:          Data Mesh:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Central Data â”‚     â”‚ Orders â”‚ â”‚ Users  â”‚ â”‚Productsâ”‚
â”‚ Team owns    â”‚     â”‚ Team   â”‚ â”‚ Team   â”‚ â”‚ Team   â”‚
â”‚ ALL data     â”‚     â”‚ owns   â”‚ â”‚ owns   â”‚ â”‚ owns   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ orders â”‚ â”‚ users  â”‚ â”‚catalog â”‚
     â†‘               â”‚ data   â”‚ â”‚ data   â”‚ â”‚ data   â”‚
  Bottleneck!        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                    Self-service Platform
                    (discovery, governance)
```

**Four principles:**
1. **Domain ownership:** Teams own their data end-to-end
2. **Data as product:** Quality, documentation, SLAs
3. **Self-service platform:** Easy to publish and discover data
4. **Federated governance:** Consistent standards, decentralized execution

**When to use:** Large organizations (50+ engineers), multiple domains, central team is bottleneck.

**When NOT to use:** Small teams, single product, moving fast.

> **ğŸ’¡ Interview Tip:** "Data mesh solves organizational scaling, not technical scaling. If your data team is a bottleneck for 10 product teams, consider it. For a startup, it's overkill."

---

## Lakehouse Architecture

**What it is:** Combines data lake (cheap storage, any format) with data warehouse (ACID, schema, performance).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Query Engines                               â”‚
â”‚              (Spark, Trino, Presto)                         â”‚
â”‚                       â”‚                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚  Table Format   â”‚                           â”‚
â”‚              â”‚ (Delta, Iceberg)â”‚                           â”‚
â”‚              â”‚ â€¢ ACID txns     â”‚                           â”‚
â”‚              â”‚ â€¢ Schema enforceâ”‚                           â”‚
â”‚              â”‚ â€¢ Time travel   â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                       â”‚                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚ Object Storage  â”‚                           â”‚
â”‚              â”‚   (S3, GCS)     â”‚                           â”‚
â”‚              â”‚ â€¢ Cheap         â”‚                           â”‚
â”‚              â”‚ â€¢ Scalable      â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why lakehouse:**
- **Cost:** Object storage is 10-100x cheaper than warehouse storage
- **Flexibility:** Store raw data, apply schema on read
- **ACID:** Delta Lake/Iceberg add transactions to files
- **One copy:** Same data for BI and ML (no ETL to warehouse)

**Key technologies:**
- **Delta Lake** (Databricks) - ACID on Spark
- **Apache Iceberg** (Netflix) - Open table format
- **Apache Hudi** (Uber) - Incremental processing

> **ğŸ’¡ Interview Tip:** "For a new analytics platform, I'd consider a lakehouse. Store everything in S3 with Delta Lake format, query with Spark/Trino. Cheaper than Snowflake for large data, and ML teams can access the same data."

---

## Auditing and Immutability

**What it is:** Append-only data storage where history is never deleted.

**Why it matters:**
- Debugging: "What happened at 3am last Tuesday?"
- Compliance: Regulators require audit trails
- Recovery: Rebuild current state from history
- Time travel: "What did this look like 6 months ago?"

```
Traditional (mutable):
UPDATE users SET balance = 100 WHERE id = 1;
-- Previous balance GONE forever

Append-only (immutable):
INSERT INTO balance_events (user_id, change, timestamp)
VALUES (1, -50, NOW());
-- Complete history preserved
```

**Implementation:** Event sourcing, CDC to immutable log (Kafka), or append-only tables.

> **ğŸ’¡ Interview Tip:** "For financial systems, I'd use append-only event logs. Every balance change is an event. Current balance = sum of events. Complete audit trail, and I can debug any discrepancy."

---

## Ethics and Privacy

**Why it's in a systems book:** As you build data systems, you have responsibility for how data is used.

**Key considerations:**
- **Collection:** Collect only what you need, with clear consent
- **Storage:** Encrypt at rest, access controls, data minimization
- **Processing:** Purpose limitation, bias auditing in ML
- **Sharing:** Anonymization is harder than you think (re-identification risk)
- **Deletion:** Right to be forgotten (GDPR), cascading deletes

**Practical point:** Design deletion into your system from the start. Immutable logs + GDPR = you need crypto-shredding or tombstones.

---

## System Design Interview Tips

### When to mention these concepts:

**Data mesh:** "At scale, how would you organize data ownership?"
**Lakehouse:** "Design a data platform for both analytics and ML"
**CDC:** "How do you keep multiple systems in sync?"
**Exactly-once:** "How do you handle failures in this pipeline?"
**Idempotency:** "What if this request is retried?"

### Key principles interviewers want to hear:

1. **Single source of truth:** Know which system is authoritative
2. **Derived data is rebuildable:** Design for this from day one
3. **Integrity over timeliness:** Rather show stale than wrong
4. **Idempotency everywhere:** Make operations safely retryable
5. **Plan for failure:** What happens when [component] fails?

### Architecture patterns to know:

```
Pattern              When to use
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Transactional outbox When you need DB write + event publish atomic
CDC + Kafka          Sync multiple data stores reliably  
Event sourcing       Audit requirements, complex domains
CQRS                 Different read/write scaling needs
Lakehouse            Large-scale analytics + ML platform
Data mesh            Large org, multiple domains
```

---

## Common Interview Questions

**Q: How do you keep a search index in sync with a database?**
A: Use CDC (Debezium) to capture changes from the database, stream to Kafka, consume and update Elasticsearch. Never dual-write from application code.

**Q: What's the difference between data lake and data warehouse?**
A: Lake = cheap storage, schema on read, raw data. Warehouse = expensive, schema enforced, optimized for queries. Lakehouse combines both with Delta Lake/Iceberg.

**Q: How do you ensure exactly-once processing in a distributed system?**
A: Idempotency keys for API requests, transactional outbox for event publishing, or checkpointing + atomic commits in stream processing.

**Q: What is data mesh?**
A: Decentralized data ownership where domain teams own their data as products. Solves organizational scaling when central data team is bottleneck.

**Q: How would you design a system that needs both real-time and batch analytics?**
A: Kappa architecture - stream everything through Kafka, process with Flink/Spark Streaming, replay from Kafka for reprocessing. Avoid Lambda's dual codebases.

**Q: What happens if one of your derived systems gets corrupted?**
A: Rebuild from source of truth. This is why identifying source of truth and keeping it correct is critical. Derived systems should always be rebuildable.

---

## The Book's Core Principles

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DDIA in Four Principles                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Reliability: Work correctly despite faults             â”‚
â”‚     â†’ Redundancy, retries, idempotency                     â”‚
â”‚                                                             â”‚
â”‚  2. Scalability: Handle growth gracefully                  â”‚
â”‚     â†’ Partitioning, replication, async processing          â”‚
â”‚                                                             â”‚
â”‚  3. Maintainability: Operable, understandable, evolvable   â”‚
â”‚     â†’ Simple designs, good abstractions, documentation     â”‚
â”‚                                                             â”‚
â”‚  4. Trade-offs: No perfect solution, only appropriate ones â”‚
â”‚     â†’ Understand the trade-offs, match to requirements     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Reference: Choosing Tools

| Need | Tool/Approach |
|------|---------------|
| OLTP | PostgreSQL, MySQL |
| Analytics | Snowflake, BigQuery, ClickHouse |
| Search | Elasticsearch, Algolia |
| Cache | Redis, Memcached |
| Event streaming | Kafka, Pulsar |
| Stream processing | Flink, Kafka Streams |
| Batch processing | Spark, dbt |
| Data lake | S3 + Delta Lake/Iceberg |
| CDC | Debezium â†’ Kafka |
| Sync multiple stores | CDC + event-driven updates |
| Audit trail | Event sourcing / append-only log |
| Large org data platform | Data mesh |
