# Chapter 8: The Trouble with Distributed Systems

## Core Concept

> "Anything that can go wrong, will go wrong." - Murphy's Law, amplified for distributed systems.

**What it is:** This chapter covers all the ways distributed systems can failâ€”and why those failures are fundamentally different from single-machine failures.

**Why it matters:** In interviews, understanding failure modes separates candidates who've "read about" distributed systems from those who've actually built them. These problems show up constantly in real systems.

**Key Insight:** In distributed systems, partial failures are the norm, not the exception. Some nodes work while others don't. You can't tell the difference between a slow network and a dead node.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Single Machine vs Distributed System                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Single Machine:                                          â”‚
â”‚   â€¢ Works or doesn't work (binary failure)                 â”‚
â”‚   â€¢ Deterministic - same input â†’ same output               â”‚
â”‚   â€¢ Shared memory, single clock                            â”‚
â”‚                                                             â”‚
â”‚   Distributed System:                                      â”‚
â”‚   â€¢ Partial failures common (some nodes up, some down)     â”‚
â”‚   â€¢ Non-deterministic (network delays vary randomly)       â”‚
â”‚   â€¢ No shared state, clocks drift apart                    â”‚
â”‚   â€¢ Messages can be lost, delayed, or duplicated           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** When discussing distributed systems, always acknowledge that "anything can fail partially." This shows maturity. Junior engineers often assume the happy path; senior engineers design for failures.

---

## Unreliable Networks

**What it is:** Networks between machines are fundamentally unreliable. Packets get lost, delayed, duplicated, or delivered out of order.

**Why it matters:** Most distributed system bugs come from assuming the network is reliable. Understanding network failures helps you design resilient systems.

### Network Problems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            When You Send a Request...                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Client â”€â”€â”€â”€â”€â”€â”€â–º ???                                      â”‚
â”‚                                                             â”‚
â”‚   What might have happened:                                â”‚
â”‚   1. Request lost in network (never arrived)               â”‚
â”‚   2. Request queued somewhere, will arrive later           â”‚
â”‚   3. Remote node received it but crashed before responding â”‚
â”‚   4. Remote node is alive but too busy to respond          â”‚
â”‚   5. Remote responded, but response was lost               â”‚
â”‚   6. Remote responded, but response is delayed             â”‚
â”‚                                                             â”‚
â”‚   âš ï¸ YOU CAN'T TELL WHICH ONE FROM THE CLIENT SIDE!       â”‚
â”‚                                                             â”‚
â”‚   All you know: "I sent a request and got no response"     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real-world implication:** If you don't get a response, you don't know if your operation succeeded. This is why idempotency mattersâ€”you might retry an operation that already succeeded.

### The Timeout Dilemma

**What it is:** How long should you wait before assuming a request failed? There's no good answer.

**Why it's hard:** You're choosing between two types of errors:
- **False positive (timeout too short):** Node was fine, just slow. You trigger unnecessary failover, which causes load spikes and cascading failures.
- **False negative (timeout too long):** Node is actually dead. Users wait forever for errors. Other services wait for you.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Timeout Trade-offs                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Short timeout (e.g., 1 second):                          â”‚
â”‚   âœ“ Fast failure detection                                 â”‚
â”‚   âœ— False positives - node was just slow (GC pause?)      â”‚
â”‚   âœ— Unnecessary failovers cause load spikes               â”‚
â”‚   âœ— Thundering herd: all clients retry at once            â”‚
â”‚                                                             â”‚
â”‚   Long timeout (e.g., 30 seconds):                         â”‚
â”‚   âœ“ Fewer false positives                                  â”‚
â”‚   âœ— Slow failure detection                                 â”‚
â”‚   âœ— Users/services wait too long                          â”‚
â”‚   âœ— Problems compound while waiting                        â”‚
â”‚                                                             â”‚
â”‚   Real-world approaches:                                   â”‚
â”‚   â€¢ Adaptive timeouts based on observed latency (P99)      â”‚
â”‚   â€¢ Phi Accrual Failure Detector (Cassandra, Akka)         â”‚
â”‚   â€¢ Exponential backoff for retries (don't hammer!)        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** When asked "how would you set timeouts?", don't give a fixed number. Say: "I'd measure the P99 latency under normal conditions and set the timeout higher than that, maybe 2-3x. Then use exponential backoff for retries to avoid thundering herd."

### Network Partitions Are Real

**What it is:** A network partition occurs when some nodes can talk to each other but not to other nodes. The network is "split."

**Why it matters:** Partitions force you to choose between consistency and availability (CAP theorem). You can't have both during a partition.

```
            Data Center 1              Data Center 2
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Node A    â”‚            â”‚   Node C    â”‚
           â”‚   Node B    â”‚     X      â”‚   Node D    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (link    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             down!)
                             
   A and B can talk to each other âœ“
   C and D can talk to each other âœ“
   But A/B cannot reach C/D! âœ—
   
   Which side has the "correct" data? Neither knows!
   
   Options during partition:
   1. Refuse writes (choose consistency - CP)
   2. Accept writes, reconcile later (choose availability - AP)
```

---

## Unreliable Clocks

**What it is:** Each machine has its own clock, and these clocks don't agree perfectly. They drift apart over time (milliseconds to seconds).

**Why it matters:** If you use timestamps to order events or determine "which write was last," clock skew can cause data loss. This is one of the most common bugs in distributed systems.

### Two Types of Clocks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Time-of-Day vs Monotonic Clocks                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Time-of-Day Clock (wall clock):                          â”‚
â”‚   â€¢ Returns current time: "2024-01-15 10:30:00.123"        â”‚
â”‚   â€¢ âš ï¸ CAN JUMP BACKWARDS! (NTP sync, leap seconds)       â”‚
â”‚   â€¢ âŒ DON'T use for measuring elapsed time               â”‚
â”‚   â€¢ âŒ DON'T use for ordering events across machines      â”‚
â”‚                                                             â”‚
â”‚   Monotonic Clock:                                         â”‚
â”‚   â€¢ Returns arbitrary counter (nanoseconds since boot)     â”‚
â”‚   â€¢ âœ“ ALWAYS moves forward on a single machine            â”‚
â”‚   â€¢ âœ“ Perfect for measuring duration locally              â”‚
â”‚   â€¢ âŒ Values meaningless across different machines       â”‚
â”‚                                                             â”‚
â”‚   Rule: Use monotonic for "how long did this take?"        â”‚
â”‚         Use wall clock only for human display              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Clock Skew and Last-Write-Wins

**What it is:** Last-Write-Wins (LWW) uses timestamps to resolve conflicts: the write with the latest timestamp wins. Sounds reasonable, but clock skew breaks it.

**Why it's dangerous:** If Node A's clock is fast, its writes will "win" even if they happened before Node B's writes in real time. This silently loses data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Last-Write-Wins Bug                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Real-world timeline:                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º      â”‚
â”‚                                                             â”‚
â”‚   Node A (clock is 5 seconds FAST):                        â”‚
â”‚   Clock shows: 10:00:05                                    â”‚
â”‚   Action: write x = 1                                      â”‚
â”‚                                                             â”‚
â”‚   Node B (clock is correct):                               â”‚
â”‚   Clock shows: 10:00:00                                    â”‚
â”‚   Action: write x = 2 (happens AFTER Node A in real time) â”‚
â”‚                                                             â”‚
â”‚   With LWW: x = 1 wins (10:00:05 > 10:00:00)              â”‚
â”‚   Reality: x = 2 was the LATER write!                     â”‚
â”‚                                                             â”‚
â”‚   âš ï¸ 5 seconds of clock skew caused SILENT DATA LOSS      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solutions:**

| Approach | How it works | Used by |
|----------|--------------|---------|
| Logical clocks (Lamport) | Counters that track causality, not time | Many systems |
| Vector clocks | Track per-node counters to detect conflicts | Riak, Dynamo |
| TrueTime | Bounded uncertainty intervals | Google Spanner |
| Hybrid Logical Clocks | Combine physical and logical time | CockroachDB |

> **ğŸ’¡ Interview Tip:** When someone mentions LWW, ask "how do you handle clock skew?" If they don't have an answer, that's a red flag. Good answers: logical clocks, vector clocks, or accepting that LWW can lose data (fine for some use cases like caching).

### Google TrueTime

**What it is:** Instead of returning a single timestamp, TrueTime returns an interval: "the current time is somewhere between T1 and T2."

**Why it works:** By explicitly acknowledging uncertainty, Spanner can wait until intervals don't overlap, guaranteeing correct ordering.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TrueTime API                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   TT.now() returns interval [earliest, latest]             â”‚
â”‚                                                             â”‚
â”‚   Example: TT.now() = [10:00:00.000, 10:00:00.007]         â”‚
â”‚   Meaning: "Current time is within this 7ms window"        â”‚
â”‚                                                             â”‚
â”‚   To guarantee ordering:                                   â”‚
â”‚   â€¢ Commit T1 at time t1                                   â”‚
â”‚   â€¢ Before committing T2, wait until TT.now().earliest    â”‚
â”‚     > t1 (intervals don't overlap)                         â”‚
â”‚   â€¢ Now T2 is guaranteed to be "after" T1                  â”‚
â”‚                                                             â”‚
â”‚   Google achieves Îµ â‰ˆ 7ms using:                           â”‚
â”‚   â€¢ GPS receivers in every data center                     â”‚
â”‚   â€¢ Atomic clocks as backup                                â”‚
â”‚   â€¢ Sophisticated time sync protocols                      â”‚
â”‚                                                             â”‚
â”‚   Trade-off: Commits have added latency (wait for Îµ)      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** TrueTime is impressive but requires specialized hardware (GPS, atomic clocks). For most companies, use logical clocks or accept eventual consistency. Only Google-scale systems justify TrueTime's complexity.

---

## Process Pauses

**What it is:** A process can pause for an arbitrary amount of time and have no idea it was paused. When it wakes up, it continues as if nothing happened.

**Why it matters:** A process holding a lock or lease might pause, have the lock/lease expire, and another process might acquire it. When the first process wakes up, it thinks it still has the lock. Two processes now think they're the leader â†’ split brain!

### Why Processes Pause

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sources of Process Pauses                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   1. Garbage Collection (GC)                               â”‚
â”‚      â€¢ Java/Go can pause for seconds during GC!           â”‚
â”‚      â€¢ "Stop-the-world" pauses freeze the entire process  â”‚
â”‚      â€¢ The process has NO IDEA it was paused              â”‚
â”‚                                                             â”‚
â”‚   2. Virtual Machine Suspension                            â”‚
â”‚      â€¢ VM live migration between physical hosts            â”‚
â”‚      â€¢ Hypervisor can pause your VM to run others         â”‚
â”‚      â€¢ Cloud instances can be "stolen" temporarily        â”‚
â”‚                                                             â”‚
â”‚   3. Operating System                                      â”‚
â”‚      â€¢ Context switches when CPU is overloaded            â”‚
â”‚      â€¢ Swapping to disk (thrashing)                       â”‚
â”‚      â€¢ Disk I/O waits                                      â”‚
â”‚                                                             â”‚
â”‚   4. Resource Contention                                   â”‚
â”‚      â€¢ Another process hogging CPU                         â”‚
â”‚      â€¢ Network socket buffers full                        â”‚
â”‚                                                             â”‚
â”‚   Key insight: The paused process CANNOT TELL it paused!  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Lease Problem

**What it is:** A lease is like a lock with an expiration time. If you don't renew it, someone else can take it. But what if you pause?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Lease Expiry During GC Pause                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Timeline:                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚
â”‚                                                             â”‚
â”‚   0s:  Node A acquires lease (expires at 10s)              â”‚
â”‚   5s:  Node A starts GC pause... (frozen)                  â”‚
â”‚   10s: Lease expires (A still paused, doesn't know!)       â”‚
â”‚   11s: Node B acquires lease (becomes new leader)          â”‚
â”‚   12s: Node A wakes up, thinks it's still the leader!      â”‚
â”‚   12s: Node A writes to storage (CORRUPTS DATA!)           â”‚
â”‚                                                             â”‚
â”‚   Problem: A had no idea 7 seconds passed!                 â”‚
â”‚   A checked the lease at 5s, it was valid.                 â”‚
â”‚   A can't tell it was paused.                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** This is a classic distributed systems gotcha. If someone proposes using "a lock with a timeout," ask "what happens if the lock holder pauses due to GC?" The answer is fencing tokens.

### Fencing Tokens

**What it is:** A monotonically increasing token issued with each lock/lease acquisition. Storage servers reject writes from stale tokens.

**Why it works:** Even if Node A wakes up thinking it has the lock, its token (33) is older than Node B's token (34). Storage rejects A's writes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Fencing Token                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Lock service returns monotonically increasing token      â”‚
â”‚                                                             â”‚
â”‚   Timeline:                                                â”‚
â”‚   1. Node A acquires lock â†’ gets token 33                  â”‚
â”‚   2. Node A pauses (GC)                                    â”‚
â”‚   3. Lock expires                                          â”‚
â”‚   4. Node B acquires lock â†’ gets token 34                  â”‚
â”‚   5. Node A wakes up with token 33                         â”‚
â”‚                                                             â”‚
â”‚   Storage server behavior:                                 â”‚
â”‚   â€¢ Tracks highest token seen (currently: 34)              â”‚
â”‚   â€¢ Node A sends write with token 33                       â”‚
â”‚   â€¢ Storage: "33 < 34? REJECTED!"                          â”‚
â”‚   â€¢ Node B's writes with token 34: ACCEPTED                â”‚
â”‚                                                             â”‚
â”‚       Node A                  Node B                       â”‚
â”‚       token=33                token=34                     â”‚
â”‚          â”‚                       â”‚                         â”‚
â”‚          â”‚    Storage Server     â”‚                         â”‚
â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                         â”‚
â”‚          â””â”€â–ºâ”‚ max_token=34  â”‚â—„â”€â”€â”€â”˜                         â”‚
â”‚     REJECT  â”‚               â”‚  ACCEPT                      â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation requirements:**
1. Lock service must return monotonically increasing tokens
2. Storage must track the highest token it's seen
3. Storage must reject writes with tokens â‰¤ the highest seen

> **ğŸ’¡ Interview Tip:** Fencing tokens are the solution to "lock expiration during pause." If designing a distributed lock, always mention: (1) locks should have TTL, (2) use fencing tokens to prevent stale holders from causing damage.

---

## Knowledge in Distributed Systems

### The Two Generals Problem

**What it is:** A thought experiment proving that you can never achieve 100% certainty of agreement over an unreliable network.

**Why it matters:** It sets realistic expectations. We can't have perfect consensus, but we can get arbitrarily close with enough redundancy.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Two Generals Problem                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Two armies must attack together or they lose.            â”‚
â”‚   They can only communicate via messengers who might       â”‚
â”‚   be captured (messages lost).                             â”‚
â”‚                                                             â”‚
â”‚   Army A                    Army B                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”                        â”‚
â”‚   â”‚     â”‚ â”€â”€"Attack atâ”€â”€â”€â–º  â”‚     â”‚                        â”‚
â”‚   â”‚     â”‚    dawn"          â”‚     â”‚  â† might be lost!     â”‚
â”‚   â”‚     â”‚ â—„â”€â”€"ACK"â”€â”€â”€â”€â”€â”€â”€   â”‚     â”‚  â† might be lost!     â”‚
â”‚   â”‚     â”‚ â”€â”€"ACK of ACK"â”€â–º  â”‚     â”‚  â† might be lost!     â”‚
â”‚   â”‚     â”‚ â—„â”€â”€"ACK of..."â”€   â”‚     â”‚  ...infinite acks     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                             â”‚
â”‚   No finite number of messages can guarantee agreement!    â”‚
â”‚                                                             â”‚
â”‚   Real-world implication:                                  â”‚
â”‚   Perfect consensus is impossible over unreliable network. â”‚
â”‚   But we CAN get "good enough" with timeouts + retries.   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Byzantine Faults

**What it is:** When a node doesn't just fail, but actively misbehavesâ€”sending incorrect or conflicting messages.

**Why it (usually) doesn't matter:** In most systems, we trust our own nodes. Byzantine fault tolerance is expensive (need 3f+1 nodes to tolerate f failures) and only matters for: blockchains, aerospace, or when you don't trust other participants.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Types of Node Failures                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Crash-stop (most common assumption):                     â”‚
â”‚   â€¢ Node works correctly, then stops forever               â”‚
â”‚   â€¢ Simple to handle: detect failure, failover             â”‚
â”‚   â€¢ Most consensus algorithms assume this                  â”‚
â”‚                                                             â”‚
â”‚   Crash-recovery:                                          â”‚
â”‚   â€¢ Node may crash but can restart                         â”‚
â”‚   â€¢ May lose in-memory state, must recover from disk       â”‚
â”‚   â€¢ Practical for real systems                             â”‚
â”‚                                                             â”‚
â”‚   Byzantine (rarely needed):                               â”‚
â”‚   â€¢ Node may behave ARBITRARILY (even maliciously)        â”‚
â”‚   â€¢ Could lie, send conflicting messages                   â”‚
â”‚   â€¢ Need 3f+1 nodes to tolerate f Byzantine failures      â”‚
â”‚   â€¢ Relevant for: Blockchain, adversarial environments    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** Most interview problems assume crash-stop failures. Don't over-engineer for Byzantine faults unless specifically asked about blockchain or adversarial scenarios.

---

## System Models

**What it is:** Formal assumptions about how the system behaves, used to reason about algorithm correctness.

### Timing Assumptions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  System Models                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Synchronous (unrealistic):                               â”‚
â”‚   â€¢ Messages delivered within known bounded time           â”‚
â”‚   â€¢ Processes execute at known bounded speed               â”‚
â”‚   â€¢ Clocks have known bounded drift                        â”‚
â”‚   â€¢ Easy to build algorithms, but doesn't match reality    â”‚
â”‚                                                             â”‚
â”‚   Asynchronous (too pessimistic):                          â”‚
â”‚   â€¢ No timing assumptions whatsoever                       â”‚
â”‚   â€¢ Can't use timeouts for failure detection!             â”‚
â”‚   â€¢ Consensus is impossible (FLP result)                   â”‚
â”‚                                                             â”‚
â”‚   Partially Synchronous (practical - what we use):        â”‚
â”‚   â€¢ System behaves synchronously MOST of the time          â”‚
â”‚   â€¢ Occasionally has unbounded delays                      â”‚
â”‚   â€¢ Algorithms use timeouts but handle false positives    â”‚
â”‚   â€¢ This matches real networks                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** Real systems assume "partial synchrony"â€”usually fast, occasionally slow. Design for the common case (use timeouts) but handle the uncommon case (retries, idempotency).

---

## Practical Patterns

### Circuit Breaker

**What it is:** A pattern that "trips" after repeated failures, preventing cascade failures by fast-failing requests.

**Why it matters:** Without circuit breakers, a failing service can take down everything that depends on it (cascade failure).

```python
class CircuitBreaker:
    """
    States: CLOSED (normal) â†’ OPEN (failing) â†’ HALF_OPEN (testing)
    """
    def __init__(self, failure_threshold=5, reset_timeout=60):
        self.failures = 0
        self.threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.state = "CLOSED"
        self.last_failure_time = None
    
    def call(self, func, *args):
        if self.state == "OPEN":
            # Check if enough time passed to try again
            if time.time() - self.last_failure_time > self.reset_timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitOpenError("Failing fast - service down")
        
        try:
            result = func(*args)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"  # Service recovered!
                self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            self.last_failure_time = time.time()
            if self.failures >= self.threshold:
                self.state = "OPEN"  # Trip the circuit
            raise
```

### Retry with Exponential Backoff

**What it is:** Wait longer between each retry attempt to avoid overwhelming a recovering service.

**Why the jitter:** Without jitter, all clients retry at the same time (thundering herd).

```python
def retry_with_backoff(func, max_retries=5, base_delay=1):
    for attempt in range(max_retries):
        try:
            return func()
        except RetriableError as e:
            if attempt == max_retries - 1:
                raise  # Final attempt failed
            
            # Exponential backoff: 1s, 2s, 4s, 8s, 16s
            delay = base_delay * (2 ** attempt)
            # Add jitter to prevent thundering herd
            jitter = random.uniform(0, delay * 0.1)
            time.sleep(delay + jitter)
```

> **ğŸ’¡ Interview Tip:** Always mention exponential backoff + jitter when discussing retries. It's a signal that you understand real-world distributed systems issues.

### Health Checks

**What it is:** Endpoints that report whether a service is alive and ready to handle traffic.

```yaml
# Kubernetes-style health probes
livenessProbe:           # Is the process alive? (restart if not)
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3    # 3 failures â†’ restart container

readinessProbe:          # Can it handle traffic? (remove from LB if not)
  httpGet:
    path: /ready
    port: 8080
  periodSeconds: 5
```

---

## The Split-Brain Problem

**What it is:** During a network partition, both sides think they're the "real" cluster and accept writes independently. When the partition heals, you have conflicting data.

**Why it's dangerous:** Two leaders accepting writes = data divergence = data loss or corruption.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Split-Brain Scenario                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Before partition:                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  [Leader A]  [Follower B]  [Follower C]  â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                             â”‚
â”‚   After network partition:                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    X    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚  [Leader A]   â”‚  (net)  â”‚ [New Leader B]â”‚             â”‚
â”‚   â”‚               â”‚  down   â”‚  [Follower C] â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                             â”‚
â”‚   Both A and B accept writes â†’ DATA DIVERGENCE!           â”‚
â”‚                                                             â”‚
â”‚   Prevention strategies:                                   â”‚
â”‚   â€¢ Quorum: Need majority to accept writes                â”‚
â”‚     (A alone = 1/3 â†’ can't write; B+C = 2/3 â†’ can write)  â”‚
â”‚   â€¢ Fencing: Cut off old leader's storage access          â”‚
â”‚   â€¢ STONITH: "Shoot The Other Node In The Head"           â”‚
â”‚     (Physically power off the old leader)                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ğŸ’¡ Interview Tip:** When designing leader election, always address split-brain. Say: "We need a quorum (majority) to elect a new leader. A single node can't declare itself leader." This shows you understand consensus.

---

## Key Takeaways

1. **Networks are unreliable** - design for packet loss, delays, partitions
2. **Clocks are unreliable** - use logical clocks for ordering, not wall clocks
3. **Processes can pause** - GC, VM migration, OS scheduling (use fencing tokens!)
4. **Partial failures are normal** - some nodes work while others don't
5. **Use timeouts** but understand they're imperfect (false positives happen)
6. **Fencing tokens** prevent stale leaders from causing damage
7. **Design for failure** - circuit breakers, retries with backoff, graceful degradation

---

## Quick Reference

| Problem | Detection | Mitigation |
|---------|-----------|------------|
| Network partition | Timeout | Quorum, graceful degradation |
| Node failure | Heartbeat timeout | Failover, replication |
| Clock skew | NTP monitoring | Logical clocks, TrueTime |
| Process pause | Lease expiry | Fencing tokens |
| Split brain | Quorum check | STONITH, fencing |
| Message loss | ACK timeout | Retry with idempotency |

---

## System Design Interview Tips

**When to bring up these problems in an interview:**
- Any distributed database design (mention clock skew, partitions)
- Leader election (mention split-brain, fencing)
- Microservices communication (mention timeouts, circuit breakers)
- Cache invalidation (mention clock skew, eventual consistency)

**Red flags interviewers look for:**
- Assuming the network is reliable
- Using wall-clock timestamps for ordering
- Not considering what happens during leader failover
- Ignoring GC pauses when discussing locks/leases

**What sets great candidates apart:**
- Proactively mentioning failure modes before being asked
- Understanding the trade-offs (timeouts: too short vs too long)
- Knowing practical solutions (fencing tokens, idempotency)
- Acknowledging uncertainty ("we can't distinguish slow from dead")

---

## Common Interview Questions

**Q: How would you detect if a node is dead?**
A: Use heartbeats with timeouts. But acknowledge that you can't distinguish "dead" from "very slow" or "network partition." Set timeout based on observed P99 latency (e.g., 2-3x P99). Use adaptive timeouts that adjust to network conditions.

**Q: How do you handle a request timeoutâ€”do you retry?**
A: It depends. If the operation is idempotent (safe to repeat), retry with exponential backoff + jitter. If not idempotent, you need an idempotency key or to check if the operation succeeded before retrying. Never hammer a failing service.

**Q: How would you implement a distributed lock?**
A: Use a lock service with TTL (auto-expiry). Issue fencing tokens with each lock grant. Storage servers must validate tokens and reject stale ones. Consider using ZooKeeper or etcd. Acknowledge that GC pauses can cause a node to hold an expired lockâ€”that's why fencing tokens matter.

**Q: Why can't you use timestamps to order events in a distributed system?**
A: Clock skew. Different machines have different clock values. A write that happened "later" in real time might have an earlier timestamp if that machine's clock is slow. Use logical clocks (Lamport, Vector) or TrueTime for ordering.

**Q: What happens if your leader fails mid-operation?**
A: Followers detect missing heartbeats and elect a new leader (quorum required). The old leader might still think it's leader (GC pause, network partition). Use fencing tokens so storage rejects writes from the stale leader. Design operations to be idempotent so they can safely retry on the new leader.
