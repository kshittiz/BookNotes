# Chapter 6: Partitioning (Sharding)

## Core Concept

> "For very large datasets or very high throughput, replication alone is not enough. We need to break data into partitions."

### What it is
Partitioning splits your data across multiple machines. Each piece of data belongs to exactly one partition (though that partition may be replicated).

### Why it matters
A single server has hard limits:
- Storage: ~10 TB practical limit
- CPU: Can't parallelize beyond cores  
- Network: Single NIC bottleneck

Partitioning breaks these limits by adding more machines.

**Terminology:**
- Partition = Shard (MongoDB) = Region (HBase) = Tablet (Bigtable) = vNode (Cassandra)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Why Partition?                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   With Partitioning:                                        â”‚
â”‚   â€¢ 1000 nodes Ã— 10 TB = 10 PB storage                    â”‚
â”‚   â€¢ Queries can run in parallel across partitions          â”‚
â”‚   â€¢ Each partition on different node = more bandwidth      â”‚
â”‚                                                             â”‚
â”‚   Goal: Spread data AND load evenly across nodes            â”‚
â”‚   Challenge: HOT SPOTS (one partition gets all traffic)    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ðŸ’¡ **Interview Tip:** Partitioning is about SCALABILITY, replication is about AVAILABILITY. They're often used together but solve different problems.

---

## Partitioning Strategies

There are two main approaches: partition by key range or by hash of key.

### 1. Key Range Partitioning

**What it is:** Assign continuous ranges of keys to partitions (like encyclopedia volumes A-C, D-F, etc.)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Key Range Partitioning                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Keys: a-zzz                                               â”‚
â”‚                                                             â”‚
â”‚   Partition 1: a-g                                         â”‚
â”‚   Partition 2: h-n                                         â”‚
â”‚   Partition 3: o-z                                         â”‚
â”‚                                                             â”‚
â”‚   Like volumes of encyclopedia!                            â”‚
â”‚                                                             â”‚
â”‚   Pros:                                                    â”‚
â”‚   âœ“ Range queries efficient (all keys a-c on same node)   â”‚
â”‚   âœ“ Easy to understand                                     â”‚
â”‚                                                             â”‚
â”‚   Cons:                                                    â”‚
â”‚   âœ— Hot spots if keys not uniformly distributed           â”‚
â”‚   âœ— Example: Timestamp keys â†’ all writes to latest partitionâ”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** HBase, Bigtable, MongoDB (before 4.4)

### Hot Spot Example (Critical!)

**The Problem:** If you partition a time-series database by timestamp, ALL current writes go to the "today" partition while historical partitions sit idle.

```
Keys: Timestamps (2024-01-01, 2024-01-02, ...)

All writes go to the "latest" partition!
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jan-Mar: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                                     â”‚
â”‚   Apr-Jun: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                                     â”‚
â”‚   Jul-Sep: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                                     â”‚
â”‚   Oct-Dec: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† HOT SPOT!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solution:** Prefix with something that distributes (like sensor_id + timestamp). Now writes spread across partitions.

> ðŸ’¡ **Interview Tip:** When asked about partition key design, ALWAYS mention hot spots. It's the #1 cause of partition-related production issues.

---

### 2. Hash Partitioning

**What it is:** Hash the key, use hash value to determine partition. Spreads data evenly regardless of key patterns.

**Why it's popular:** Eliminates hot spots from key patterns. "user_1", "user_2", "user_3" hash to completely different values.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Hash Partitioning                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   partition = hash(key) % num_partitions                   â”‚
â”‚                                                             â”‚
â”‚   Key "user_123" â†’ hash â†’ 2847291 â†’ 2847291 % 4 = 3       â”‚
â”‚   Key "user_456" â†’ hash â†’ 9182736 â†’ 9182736 % 4 = 0       â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ Part 0 â”‚ â”‚ Part 1 â”‚ â”‚ Part 2 â”‚ â”‚ Part 3 â”‚             â”‚
â”‚   â”‚user_456â”‚ â”‚user_789â”‚ â”‚user_012â”‚ â”‚user_123â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                             â”‚
â”‚   Pros:                                                    â”‚
â”‚   âœ“ Even distribution (no hot spots)                       â”‚
â”‚   âœ“ Works for any key type                                â”‚
â”‚                                                             â”‚
â”‚   Cons:                                                    â”‚
â”‚   âœ— Range queries require scatter-gather to ALL partitions â”‚
â”‚   âœ— Changing partition count = rehash everything           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** Cassandra, DynamoDB, MongoDB (default), Redis Cluster

### Trade-off Summary

| | Range Partitioning | Hash Partitioning |
|---|---|---|
| **Range queries** | âœ… Efficient (single partition) | âŒ Scatter-gather (all partitions) |
| **Even distribution** | âŒ Depends on key patterns | âœ… Always even |
| **Hot spots** | âŒ Common problem | âœ… Rare (unless one key is hot) |
| **Use when** | Need range scans (time-series) | Even distribution is priority |

---

### Consistent Hashing

**What it is:** A way to add/remove nodes without rehashing everything. Keys and nodes both map to a ring; keys belong to the next node clockwise.

**Why it matters:** With simple `hash % N`, changing N moves MOST keys. With consistent hashing, only K/N keys move on average.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Consistent Hashing                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Hash ring (0 to 2^32):                                   â”‚
â”‚                                                             â”‚
â”‚                    Node A                                   â”‚
â”‚                      â”‚                                      â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚         Node D                Node B                        â”‚
â”‚            â”‚                   â”‚                           â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                    Node C                                   â”‚
â”‚                                                             â”‚
â”‚   Key "user_123" â†’ hash â†’ walks clockwise to find node     â”‚
â”‚                                                             â”‚
â”‚   Adding/removing node only affects neighboring partitions â”‚
â”‚   (not complete rehash!)                                   â”‚
â”‚                                                             â”‚
â”‚   Virtual nodes: Each physical node owns multiple points   â”‚
â”‚   on ring for better balance                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ðŸ’¡ **Interview Tip:** "Consistent hashing" is technically different from what databases like Cassandra use. They use fixed partitions assigned to nodes. Know the distinction if asked.

---

### Compound/Composite Keys (Best of Both Worlds)

**What it is:** Hash the first part of the key (for distribution), then range-partition within each hash bucket (for efficient scans).

**This is the Cassandra pattern - memorize it!**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Cassandra Compound Key Strategy                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   CREATE TABLE posts (                                      â”‚
â”‚     user_id UUID,           -- Partition key (hashed)      â”‚
â”‚     post_time TIMESTAMP,    -- Clustering column (sorted)  â”‚
â”‚     content TEXT,                                          â”‚
â”‚     PRIMARY KEY (user_id, post_time)                       â”‚
â”‚   );                                                        â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Partition (user_123)                                â”‚  â”‚
â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚   â”‚ â”‚ 2024-01-01 09:00 â”‚ Hello world                  â”‚ â”‚  â”‚
â”‚   â”‚ â”‚ 2024-01-01 10:00 â”‚ Another post                 â”‚ â”‚  â”‚
â”‚   â”‚ â”‚ 2024-01-02 08:00 â”‚ Good morning                 â”‚ â”‚  â”‚
â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚   Result:                                                  â”‚
â”‚   â€¢ All posts for user_123 on same partition              â”‚
â”‚   â€¢ Posts sorted by time within partition                 â”‚
â”‚   â€¢ Efficient: "Get user_123's posts from Jan 2024"       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DynamoDB equivalent:** Partition Key (PK) + Sort Key (SK). Same concept, different names.

---

## Secondary Indexes with Partitioning

Secondary indexes are tricky with partitioned data because the index doesn't align with the partition key.

### Two Approaches:

| | Local Index | Global Index |
|---|---|---|
| **Structure** | Each partition indexes its own data | Index is partitioned separately |
| **Writes** | âœ… Fast (update one partition) | âŒ Slow (may update many partitions) |
| **Reads** | âŒ Scatter-gather (query all partitions) | âœ… Fast (query one index partition) |
| **Use when** | Write-heavy workloads | Read-heavy on secondary attributes |

### Document-Partitioned (Local) Index

**What it is:** Each partition maintains its own index covering only its local data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Local Index (Document-Partitioned)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Each partition has its own index for its data            â”‚
â”‚                                                             â”‚
â”‚   Partition 0                    Partition 1                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Data: cars 0-499  â”‚         â”‚ Data: cars 500-999â”‚      â”‚
â”‚   â”‚ Index: color=red  â”‚         â”‚ Index: color=red  â”‚      â”‚
â”‚   â”‚   â†’ [car_123]     â”‚         â”‚   â†’ [car_567]     â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚   Query: "Find all red cars"                               â”‚
â”‚   â†’ Must query ALL partitions (scatter-gather)             â”‚
â”‚   â†’ Combine results                                        â”‚
â”‚                                                             â”‚
â”‚   Pros: Writes update only one partition                   â”‚
â”‚   Cons: Reads may need to hit all partitions (expensive!)  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** MongoDB, Cassandra, Elasticsearch, SolrCloud

### Term-Partitioned (Global) Index

**What it is:** The index itself is partitioned by the indexed term. All "color=red" entries go to one index partition.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Global Index (Term-Partitioned)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Index is also partitioned (by index term)                â”‚
â”‚                                                             â”‚
â”‚   Index Partition A-M            Index Partition N-Z       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ color=blue â†’      â”‚         â”‚ color=red â†’       â”‚      â”‚
â”‚   â”‚   [car_1, car_99] â”‚         â”‚   [car_5, car_42] â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚   Query: "Find all red cars"                               â”‚
â”‚   â†’ Query only the "r" index partition                     â”‚
â”‚   â†’ Single partition read!                                 â”‚
â”‚                                                             â”‚
â”‚   Pros: Efficient reads (single partition for a term)      â”‚
â”‚   Cons: Writes may update multiple index partitions        â”‚
â”‚         (often async to avoid slow writes)                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** DynamoDB Global Secondary Indexes, Riak search

> ðŸ’¡ **Interview Tip:** When asked about secondary indexes in a distributed database, mention this trade-off. There's no free lunch - either writes or reads will be expensive.

---

## Rebalancing Partitions

### What it is
Moving data between nodes to maintain even distribution as nodes are added/removed or data grows.

### Why it matters
Without proper rebalancing, you end up with overloaded nodes while others sit idle.

### Why NOT to Use: hash(key) mod N

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        DON'T: hash(key) mod N                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   N=4: hash(key) % 4 = partition                           â”‚
â”‚   N=5: hash(key) % 5 = partition  â† Everything moves!      â”‚
â”‚                                                             â”‚
â”‚   Changing N requires moving MOST data!                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This is the classic mistake.** If you go from 4 to 5 nodes, almost every key changes which partition it belongs to.

### The Right Way: Fixed Number of Partitions

**What it is:** Create way more partitions than nodes upfront. When adding nodes, just move whole partitions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Fixed number of partitions                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Create 1000 partitions upfront (more than nodes)         â”‚
â”‚                                                             â”‚
â”‚   4 nodes: each has 250 partitions                         â”‚
â”‚   Add node: steal partitions from others                   â”‚
â”‚                                                             â”‚
â”‚   Node 1: [0-249]    â†’    Node 1: [0-199]                  â”‚
â”‚   Node 2: [250-499]  â†’    Node 2: [250-399]                â”‚
â”‚   Node 3: [500-749]  â†’    Node 3: [500-599]                â”‚
â”‚   Node 4: [750-999]  â†’    Node 4: [750-849]                â”‚
â”‚                           Node 5: [200-249,400-449,...]    â”‚
â”‚                                                             â”‚
â”‚   Only partition assignment changes, not partitioning!     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** Elasticsearch, Couchbase, Riak, Voldemort

**Trade-off:** Must guess partition count upfront. Too few = large partitions that are slow to move. Too many = overhead per partition.

### Dynamic Partitioning

**What it is:** Start small, split partitions when they get too big, merge when they shrink.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Dynamic Partitioning                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Start with few partitions, split as they grow            â”‚
â”‚                                                             â”‚
â”‚   Initial:      After growth:     After more growth:       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”           â”‚
â”‚   â”‚  A  â”‚   â†’   â”‚  A  â”‚  B  â”‚  â†’  â”‚A1â”‚A2â”‚B1â”‚B2â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜           â”‚
â”‚                                                             â”‚
â”‚   Split threshold: ~10GB per partition (HBase)             â”‚
â”‚   Merge threshold: Below half of split size                â”‚
â”‚                                                             â”‚
â”‚   Good for key-range partitioning                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** HBase, RethinkDB, MongoDB

### Rebalancing Strategy Comparison

| Strategy | Pros | Cons | Used By |
|----------|------|------|---------|
| Fixed partitions | Simple, predictable | Must guess count upfront | Elasticsearch, Riak |
| Dynamic split/merge | Adapts to data | More complex, initial bottleneck | HBase, MongoDB |
| Consistent hashing | Minimal data movement | Uneven distribution without vnodes | Cassandra |

---

## Request Routing

### The Problem
Client has a key. Which node holds that key's partition?

### Three Approaches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Routing Approaches                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   1. Client-side routing                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ Client â”‚ â”€â”€â”€ knows partition mapping                   â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     directly contacts correct node            â”‚
â”‚       â”‚                                                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Node 3 (has the partition)              â”‚
â”‚                                                             â”‚
â”‚   2. Routing tier (partition-aware load balancer)          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚ Client â”‚â”€â”€â”€â–ºâ”‚ Router/LB   â”‚â”€â”€â”€â–º Node 3                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚(partition-  â”‚                           â”‚
â”‚                 â”‚ aware)      â”‚                            â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                             â”‚
â”‚   3. Contact any node (gossip-based)                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ Client â”‚â”€â”€â”€â–º Node 1 â”€â”€(forwards)â”€â”€â–º Node 3            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Approach | Pros | Cons | Used By |
|----------|------|------|---------|
| Client-aware | Lowest latency | Client complexity | Cassandra drivers |
| Routing tier | Simple clients | Extra hop | MongoDB (mongos) |
| Any-node | Simple everything | Extra hop + gossip overhead | Cassandra, Riak |

### Coordination Service (ZooKeeper Pattern)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ZooKeeper for Partition Metadata                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                 â”‚  ZooKeeper  â”‚                            â”‚
â”‚                 â”‚             â”‚                            â”‚
â”‚                 â”‚ partition 0 â”‚                            â”‚
â”‚                 â”‚  â†’ node 1   â”‚                            â”‚
â”‚                 â”‚ partition 1 â”‚                            â”‚
â”‚                 â”‚  â†’ node 2   â”‚                            â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚            subscribe   â”‚   register                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚          â”‚                           â”‚                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  Client/  â”‚              â”‚   Nodes    â”‚             â”‚
â”‚    â”‚  Router   â”‚              â”‚ (register  â”‚             â”‚
â”‚    â”‚ (watches) â”‚              â”‚  partition â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  ownership)â”‚             â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                             â”‚
â”‚   When partition moves: ZK notifies clients automatically  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** HBase, Kafka, SolrCloud

### Gossip Protocol (No Coordinator)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Gossip Protocol                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Each node knows full partition map                       â”‚
â”‚   Nodes periodically exchange state with random peers      â”‚
â”‚                                                             â”‚
â”‚   Node 1 â—„â”€â”€â”€â”€â”€â”€â–º Node 2                                   â”‚
â”‚     â”‚                â”‚                                      â”‚
â”‚     â”‚                â”‚                                      â”‚
â”‚     â–¼                â–¼                                      â”‚
â”‚   Node 3 â—„â”€â”€â”€â”€â”€â”€â–º Node 4                                   â”‚
â”‚                                                             â”‚
â”‚   Changes propagate eventually (O(log N) rounds)           â”‚
â”‚                                                             â”‚
â”‚   Client contacts any node â†’ node routes if needed         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Used by:** Cassandra, Riak

---

## Practical Examples

### DynamoDB Partition Design

```python
# Good partition key: High cardinality, evenly distributed
table.put_item(Item={
    'PK': f'USER#{user_id}',           # Partition key
    'SK': f'ORDER#{order_id}',          # Sort key
    'OrderDate': '2024-01-15',
    'Total': 99.99
})

# Query: All orders for a user (single partition)
table.query(
    KeyConditionExpression='PK = :pk AND begins_with(SK, :sk)',
    ExpressionAttributeValues={
        ':pk': 'USER#123',
        ':sk': 'ORDER#'
    }
)

# Anti-pattern: Date as partition key (hot partition!)
# 'PK': '2024-01-15'  # All today's data on one partition
```

### MongoDB Sharding Setup

```javascript
// Enable sharding on database
sh.enableSharding("mydb")

// Shard collection with hashed key (even distribution)
sh.shardCollection("mydb.users", { "_id": "hashed" })

// Or range-based for time-series with compound key
sh.shardCollection("mydb.events", { 
  "tenant_id": 1,      // Partition by tenant first
  "timestamp": 1        // Then range within tenant
})

// Check distribution
db.users.getShardDistribution()
```

---

## Hot Spot Mitigation

**The celebrity problem:** Even with hash partitioning, one key can be extremely hot. A celebrity's post gets millions of writes (likes, comments) all hitting one partition.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Handling Hot Partitions                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Problem: Celebrity posts (millions of likes)             â”‚
â”‚   Solution: Add random suffix to split load                â”‚
â”‚                                                             â”‚
â”‚   Instead of:  post_123                                    â”‚
â”‚   Use:         post_123_0, post_123_1, ... post_123_99    â”‚
â”‚                                                             â”‚
â”‚   Writes: Randomly pick suffix (spread across partitions)  â”‚
â”‚   Reads:  Query all 100 keys, aggregate                   â”‚
â”‚                                                             â”‚
â”‚   Trade-off: Faster writes, slower reads                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real-world approach:** Maintain a list of "hot" keys (detected via metrics). Only apply key-splitting to those, not everything.

> ðŸ’¡ **Interview Tip:** Hot spots show you understand that partitioning isn't magic. Even with perfect distribution strategy, application access patterns can create hot spots.

---

## Key Takeaways

1. **Hash partitioning** for even distribution, **range** for ordered access
2. **Compound keys** give best of both (Cassandra/DynamoDB pattern)
3. **Local indexes** = fast writes, scatter-gather reads
4. **Global indexes** = fast reads, complex writes
5. **Fixed partitions** easier than dynamic for rebalancing
6. **Don't use mod N** - causes massive data movement
7. **Hot spots** happen even with hashing - design partition keys carefully

---

## Quick Reference

| Database | Default Partitioning | Rebalancing |
|----------|---------------------|-------------|
| Cassandra | Hash (vnodes) | Automatic |
| MongoDB | Hash or Range | Automatic |
| DynamoDB | Hash | Automatic (hidden) |
| HBase | Range | Dynamic split/merge |
| Elasticsearch | Hash (fixed count) | Manual shard allocation |
| Kafka | Hash by key | Manual partition reassign |

---

## System Design Interview Tips

### Choosing a Partition Key

Ask these questions:
1. **What's my access pattern?** (point lookups vs range scans)
2. **What's the cardinality?** (high cardinality = good distribution)
3. **Are there hot keys?** (celebrity problem)
4. **Do I need efficient range queries?** (consider compound keys)

### Common Design Patterns

**User data:** Partition by `user_id` (high cardinality, even access)

**Time-series:** Partition by `device_id + timestamp` (avoid all-writes-to-today problem)

**Multi-tenant SaaS:** Partition by `tenant_id` (isolates tenants, but watch for large tenants)

### Red Flags to Avoid

- Using timestamp alone as partition key
- Using low-cardinality fields (country, status)
- Ignoring secondary index costs
- Assuming partition count never needs to change

---

## Common Interview Questions

**Q: Design a URL shortener. How would you partition the data?**
> Hash partition by the short URL (high cardinality, even access pattern). Use fixed number of partitions for easy rebalancing. No need for range queries.

**Q: How would you handle a "hot" partition for a viral post?**
> Add random suffix to the key (e.g., `post_123_0` through `post_123_99`). Writes distribute randomly across 100 keys. Reads query all 100 and aggregate. Only do this for detected hot keys.

**Q: What's the difference between local and global secondary indexes?**
> Local: index lives with data partition. Fast writes, but reads must scatter-gather all partitions. Global: index is partitioned by index term. Fast reads for index queries, but writes may touch multiple partitions.

**Q: DynamoDB vs Cassandra - how do their partition strategies differ?**
> Both use hash-based partitioning with partition key + sort key. DynamoDB hides partition management entirely. Cassandra uses vnodes (virtual nodes) and gossip protocol. Both support compound keys for efficient within-partition range queries.

**Q: How do you decide on the number of partitions?**
> Consider: expected data size, query throughput, node count. Rule of thumb: partitions should be 100MB-1GB. Too few = large partitions slow to rebalance. Too many = metadata overhead. Many systems use dynamic partitioning to avoid this choice.
